{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:51:30.496857Z",
     "start_time": "2020-08-30T02:51:30.490367Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, path):\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        \n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "\n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        ratingList = list(zip(df.userid.tolist(), df.itemid.tolist()))\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        negativeList = df.iloc[:, 1:].values.tolist()\n",
    "        return negativeList\n",
    "    \n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        df = pd.read_csv(filename, sep=\"\\t\")\n",
    "        num_users = df.userid.max()\n",
    "        num_items = df.itemid.max()\n",
    "        mat = sp.dok_matrix((num_users + 1, num_items + 1), dtype=np.float32)\n",
    "        interactions = df[['userid', 'itemid']].values.tolist()\n",
    "        # [(0, 2969), (0, 1178), (0, 1574), (0, 957)]\n",
    "        for user, item in interactions:\n",
    "            mat[user, item] = 1.\n",
    "        # [((0, 2969), 1.0), ((0, 1178), 1.0), ((0, 1574), 1.0), ((0, 957), 1.0)]\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Javier/ml-1m.test.negative\n",
      "    positive  item_n0  item_n1  item_n2  item_n3  item_n4  item_n5  item_n6  \\\n",
      "0    (0, 47)     3184     3478     1694     3377      340     2111     2833   \n",
      "1  (1, 1737)      683       15      369     1630      777      899     2465   \n",
      "2  (2, 1900)      478     3184     2477     1061     2196      416      976   \n",
      "3  (3, 1774)     1085     2309     1203     3366     3661     3241     2349   \n",
      "4   (4, 279)      476     1002      596     2478     2382     2791      704   \n",
      "\n",
      "   item_n7  item_n8  ...  item_n89  item_n90  item_n91  item_n92  item_n93  \\\n",
      "0      382     2152  ...      2486       428       416       799      2623   \n",
      "1     2906     2175  ...       235      3515      3255      1878      2017   \n",
      "2     1623     3263  ...      1492      1994      2294      3431      2735   \n",
      "3     2363     2903  ...      1768       752      2217      1222        86   \n",
      "4     1097      624  ...      1527        78       958      2780      3703   \n",
      "\n",
      "   item_n94  item_n95  item_n96  item_n97  item_n98  \n",
      "0      3346       956       294      2309      1867  \n",
      "1       144      2000      2894        27      1146  \n",
      "2      2996      3442      1587       277      2588  \n",
      "3      1469      3074       479       838      1824  \n",
      "4      3261      3343      1358      3610      2382  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T02:51:32.363445Z",
     "start_time": "2020-08-30T02:51:32.356689Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, topK):\n",
    "\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _topK\n",
    "\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _topK = topK\n",
    "\n",
    "    hits, ndcgs = [],[]\n",
    "    for idx in range(len(_testRatings)):\n",
    "        (hr,ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    return (hits, ndcgs)\n",
    "\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = _model.predict([users, np.array(items)],\n",
    "                                 batch_size=100, verbose=0)\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    items.pop()\n",
    "\n",
    "    ranklist = heapq.nlargest(_topK, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def get_train_instances(train, n_items, n_neg, testNegatives):\n",
    "    user, item, labels = [],[],[]\n",
    "    n_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user.append(u)\n",
    "        item.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances: we also need to make sure they are not in the\n",
    "        # test dataset\n",
    "        for t in range(n_neg):\n",
    "            j = np.random.randint(n_items)\n",
    "            while ((u, j) in train.keys()) or (j in testNegatives[u]):\n",
    "                j = np.random.randint(n_items)\n",
    "            user.append(u)\n",
    "            item.append(j)\n",
    "            labels.append(0)\n",
    "    return np.array(user), np.array(item), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-30T03:20:23.077214Z",
     "start_time": "2020-08-30T02:51:36.958958Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: 81.23s, HR = 0.5623, NDCG = 0.3145, loss = 0.3356, validated in 11.75s\n",
      "Iteration 2: 80.64s, HR = 0.6005, NDCG = 0.3359, loss = 0.2996, validated in 11.56s\n",
      "Iteration 3: 80.51s, HR = 0.6063, NDCG = 0.3403, loss = 0.2922, validated in 12.16s\n",
      "Iteration 4: 81.14s, HR = 0.6056, NDCG = 0.3404, loss = 0.2899, validated in 11.77s\n",
      "Iteration 5: 80.82s, HR = 0.6238, NDCG = 0.3542, loss = 0.2852, validated in 11.59s\n",
      "Iteration 6: 81.33s, HR = 0.6252, NDCG = 0.3553, loss = 0.2827, validated in 11.77s\n",
      "Iteration 7: 80.70s, HR = 0.6233, NDCG = 0.3528, loss = 0.2818, validated in 11.49s\n",
      "Iteration 8: 80.91s, HR = 0.6432, NDCG = 0.3646, loss = 0.2817, validated in 11.76s\n",
      "Iteration 9: 81.02s, HR = 0.6301, NDCG = 0.3608, loss = 0.2813, validated in 11.92s\n",
      "Iteration 10: 80.85s, HR = 0.6320, NDCG = 0.3624, loss = 0.2813, validated in 11.53s\n",
      "Iteration 11: 80.80s, HR = 0.6407, NDCG = 0.3657, loss = 0.2817, validated in 11.42s\n",
      "Iteration 12: 79.89s, HR = 0.6040, NDCG = 0.3455, loss = 0.2820, validated in 11.60s\n",
      "Iteration 13: 80.71s, HR = 0.6174, NDCG = 0.3539, loss = 0.2824, validated in 11.67s\n",
      "Iteration 14: 80.40s, HR = 0.6379, NDCG = 0.3644, loss = 0.2827, validated in 11.93s\n",
      "Iteration 15: 81.65s, HR = 0.6185, NDCG = 0.3513, loss = 0.2832, validated in 11.58s\n",
      "Iteration 16: 80.08s, HR = 0.6212, NDCG = 0.3500, loss = 0.2834, validated in 11.49s\n",
      "Iteration 17: 80.52s, HR = 0.6374, NDCG = 0.3661, loss = 0.2840, validated in 11.64s\n",
      "Iteration 18: 81.20s, HR = 0.6298, NDCG = 0.3636, loss = 0.2845, validated in 11.43s\n",
      "Iteration 19: 79.73s, HR = 0.6336, NDCG = 0.3606, loss = 0.2849, validated in 11.63s\n",
      "Iteration 20: 79.06s, HR = 0.6298, NDCG = 0.3549, loss = 0.2853, validated in 11.72s\n",
      "End. Best Iteration 8:  HR = 0.6432, NDCG = 0.3646. \n",
      "The best GMF model is saved to models/keras_GMF_bs_256_reg_00_lr_001_n_emb_8.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import heapq\n",
    "import tensorflow.keras\n",
    "import multiprocessing as mp\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, Flatten, multiply\n",
    "from tensorflow.keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from time import time\n",
    "\n",
    "def GMF(n_users, n_items, n_emb, reg):\n",
    "    user = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "    # user and item embeddings\n",
    "    # [?, 1, 8]\n",
    "    MF_Embedding_User = Embedding(\n",
    "        input_dim=n_users,\n",
    "        output_dim=n_emb,\n",
    "        name='user_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "    # [?, 1, 8]\n",
    "    MF_Embedding_Item = Embedding(\n",
    "        input_dim=n_items,\n",
    "        output_dim=n_emb,\n",
    "        name='item_embedding',\n",
    "        embeddings_initializer='normal',\n",
    "        embeddings_regularizer=l2(reg),\n",
    "        input_length=1)\n",
    "\n",
    "    # Flatten and multiply\n",
    "    # [?, 8]\n",
    "    user_latent = Flatten()(MF_Embedding_User(user))\n",
    "    # [?, 8]\n",
    "    item_latent = Flatten()(MF_Embedding_Item(item))\n",
    "    # [?, 8]\n",
    "    predict_vector = multiply([user_latent, item_latent])\n",
    "\n",
    "    # output layer\n",
    "    prediction = Dense(1, activation='sigmoid',\n",
    "                       kernel_regularizer=l2(reg),\n",
    "                       kernel_initializer='lecun_uniform',\n",
    "                       name='prediction')(predict_vector)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=[user, item], outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datadir = \"Data_Javier/\"\n",
    "    dataname = \"ml-1m\"\n",
    "    modeldir = \"models\"\n",
    "    n_emb = 8\n",
    "    reg = 0.0\n",
    "    batch_size = 256\n",
    "    epochs = 20\n",
    "    learner = \"adam\"\n",
    "    lr = 0.01\n",
    "    validate_every = 1\n",
    "    save_model = True\n",
    "    topK = 10\n",
    "    n_neg = 4\n",
    "\n",
    "    modelfname = \"keras_GMF\" + \\\n",
    "                 \"_\".join([\"_bs\", str(batch_size)]) + \\\n",
    "                 \"_\".join([\"_reg\", str(reg).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_lr\", str(lr).replace(\".\", \"\")]) + \\\n",
    "                 \"_\".join([\"_n_emb\", str(n_emb)]) + \".h5\"\n",
    "    modelpath = os.path.join(modeldir, modelfname)\n",
    "    resultsdfpath = os.path.join(modeldir, 'results_df.p')\n",
    "\n",
    "    dataset = Dataset(os.path.join(datadir, dataname))\n",
    "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    n_users, n_items = train.shape\n",
    "\n",
    "    model = GMF(n_users, n_items, n_emb, reg)\n",
    "    if learner.lower() == \"adagrad\":\n",
    "        model.compile(optimizer=Adagrad(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"rmsprop\":\n",
    "        model.compile(optimizer=RMSprop(lr=lr), loss='binary_crossentropy')\n",
    "    elif learner.lower() == \"adam\":\n",
    "        model.compile(optimizer=Adam(lr=lr), loss='binary_crossentropy')\n",
    "    else:\n",
    "        model.compile(optimizer=SGD(lr=lr), loss='binary_crossentropy')\n",
    "\n",
    "    best_hr, best_ndcg, best_iter = 0, 0, 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t1 = time()\n",
    "        user, item, labels = get_train_instances(train, n_items, n_neg, testNegatives)\n",
    "        hist = model.fit([user, item], labels, batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "        t2 = time()\n",
    "        if epoch % validate_every == 0:\n",
    "            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK)\n",
    "            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "            print(\"Iteration {}: {:.2f}s, HR = {:.4f}, NDCG = {:.4f}, loss = {:.4f}, validated in {:.2f}s\"\n",
    "                  .format(epoch, t2 - t1, hr, ndcg, loss, time() - t2))\n",
    "            if hr > best_hr:\n",
    "                best_hr, best_ndcg, best_iter, train_time = hr, ndcg, epoch, t2 - t1\n",
    "                if save_model:\n",
    "                    model.save_weights(modelpath, overwrite=True)\n",
    "\n",
    "    print(\"End. Best Iteration {}:  HR = {:.4f}, NDCG = {:.4f}. \"\n",
    "          .format(best_iter, best_hr, best_ndcg))\n",
    "    if save_model:\n",
    "        print(\"The best GMF model is saved to {}\".format(modelpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
